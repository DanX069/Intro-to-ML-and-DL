{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Question 1:\nGenerate a dataset for linear regression with 1000 samples, 5 features and single target.\n\nVisualize the data by plotting the target column against each feature column. Also plot the best fit line in each case.\n\nHint : search for obtaining regression line using numpy.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Set a random seed for reproducibility\nnp.random.seed(0)\n\n# Number of samples\nn_samples = 1000\n\n# Create random feature data\nX = np.random.rand(n_samples, 5)\n\n# Create coefficients for our features\ncoefficients = np.array([1, 2, 3, 4, 5])\n\n# Create target variable based on the coefficients and features\ny = X.dot(coefficients)\n\n# Combine features and target into a single DataFrame\ndf = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(X.shape[1])])\ndf['target'] = y\n\n# Print the first few rows of the DataFrame\nprint(df.head())\nprint(df.shape)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-21T17:17:36.886667Z","iopub.execute_input":"2023-05-21T17:17:36.887116Z","iopub.status.idle":"2023-05-21T17:17:36.893898Z","shell.execute_reply.started":"2023-05-21T17:17:36.887075Z","shell.execute_reply":"2023-05-21T17:17:36.892791Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfor i in range(1, 6):\n    # Extract feature and target\n    feature = df[f'feature_{i}']\n    target = df['target']\n\n    # Use polyfit to fit a 1-degree polynomial (a line) to the data\n    coefficients = np.polyfit(feature, target, deg=1)\n\n    # Create a 1d polynomial function from the coefficients\n    poly = np.poly1d(coefficients)\n\n    # Create a new figure\n    plt.figure(figsize=(8, 6))\n\n    # Scatter plot of the actual data\n    plt.scatter(feature, target, label='Actual Data')\n\n    # Line plot of the best fit line\n    plt.plot(feature, poly(feature), color='red', label='Best Fit Line')\n\n    # Title and labels\n    plt.title(f'Feature {i} vs Target with Best Fit Line')\n    plt.xlabel(f'Feature {i}')\n    plt.ylabel('Target')\n\n    # Legend location\n    plt.legend(loc='upper left')\n\n    # Show the plot\n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 2:\nMake a classification dataset of 1000 samples with 2 features, 2 classes and 2 clusters per class.\nPlot the data.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate the dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_classes=2, n_clusters_per_class=2, random_state=1)\n\n# Visualize the data\nplt.figure(figsize=(10, 7))\n\n# Use a different color for each class\ncolors = ['red', 'blue']\n\nfor i, color in enumerate(colors):\n    plt.scatter(X[y == i, 0], X[y == i, 1], c=color, label=f'Class {i+1}', edgecolor='black')\n\nplt.title('Classification Data with 2 Features, 2 Classes and 2 Clusters per Class')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:17:36.895736Z","iopub.execute_input":"2023-05-21T17:17:36.897126Z","iopub.status.idle":"2023-05-21T17:17:36.913224Z","shell.execute_reply.started":"2023-05-21T17:17:36.897084Z","shell.execute_reply":"2023-05-21T17:17:36.911734Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"# Question 3:\nMake a clustering dataset with 2 features and 4 clusters.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n# Generate the dataset\nX, y = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)\n\n# Visualize the data\nplt.figure(figsize=(10, 7))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\nplt.title('Clustering Data with 2 Features and 4 Clusters')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:17:36.914547Z","iopub.execute_input":"2023-05-21T17:17:36.915435Z","iopub.status.idle":"2023-05-21T17:17:36.934255Z","shell.execute_reply.started":"2023-05-21T17:17:36.915401Z","shell.execute_reply":"2023-05-21T17:17:36.932682Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"# Question 4\nGo to the website https://www.worldometers.info/coronavirus/ and scrape the table containing covid-19 infection and deaths data using requests and BeautifulSoup.  Convert the table to a Pandas dataframe with the following columns : Country, Continent, Population, TotalCases, NewCases, TotalDeaths, NewDeaths,TotalRecovered, NewRecovered,  ActiveCases.\n\n*(<b>Optional Challenge :</b> Change the data type of the Columns (Population ... till ActiveCases) to integer. For that you need to remove the commas and plus signs. You may need to use df.apply() and pd.to_numeric() . Take care of the values which are empty strings.)","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Request the page\nurl = 'https://www.worldometers.info/coronavirus/'\nr = requests.get(url)\nsoup = BeautifulSoup(r.text, 'html.parser')\n\n# Find the table\ntable = soup.find(\"table\", attrs={\"id\": \"main_table_countries_today\"})\n\n# Find all rows\ntable_data = table.tbody.find_all(\"tr\") \n\n# Create a dictionary to hold the data\ndata = {\n    'Country': [],\n    'Population': [],\n    'TotalCases': [],\n    'NewCases': [],\n    'TotalDeaths': [],\n    'NewDeaths': [],\n    'TotalRecovered': [],\n    'NewRecovered': [],\n    'ActiveCases': []\n}\n\n# Loop through each row\nfor row in table_data:\n    cols = row.find_all('td')\n    if cols[1].text.strip() not in data['Country']:\n        data['Country'].append(cols[1].text.strip())\n        data['Population'].append(cols[14].text.strip())\n        data['TotalCases'].append(cols[2].text.strip())\n        data['NewCases'].append(cols[3].text.strip())\n        data['TotalDeaths'].append(cols[4].text.strip())\n        data['NewDeaths'].append(cols[5].text.strip())\n        data['TotalRecovered'].append(cols[6].text.strip())\n        data['NewRecovered'].append(cols[7].text.strip())\n        data['ActiveCases'].append(cols[8].text.strip())\n\n# Create the dataframe\ndf = pd.DataFrame(data)\n\n# Clean the data and convert to integers\nfor col in ['Population', 'TotalCases', 'NewCases', 'TotalDeaths', 'NewDeaths', 'TotalRecovered', 'NewRecovered', 'ActiveCases']:\n    df[col] = df[col].replace({',': '', '\\+': '', 'N/A': '0'}, regex=True)  # Remove commas and plus signs\n    df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, turning errors into NaN\n\n# Print the dataframe\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:17:36.937383Z","iopub.execute_input":"2023-05-21T17:17:36.938005Z","iopub.status.idle":"2023-05-21T17:17:36.947039Z","shell.execute_reply.started":"2023-05-21T17:17:36.937969Z","shell.execute_reply":"2023-05-21T17:17:36.945264Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"# Question 5\n\nGenerate an imbalanced classification dataset using sklearn of 1000 samples with 2 features, 2 classes and 1 cluster per class. Plot the data. One of the class should contain only 5% of the total samples. Confirm this either using numpy or Counter. Plot the data.\n\nNow oversample the minority class to 5 times its initial size using SMOTE. Verify the number. Plot the data.\n\nNow undersample the majority class to 3 times the size of minority class using RandomUnderSampler. Verify the number. Plot the data.\n\nReference : Last markdown cell of the examples.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# create dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n                           n_redundant=0, n_clusters_per_class=1, weights=[0.95])\n\n# plot the dataset\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n            s=25, edgecolor='k')\n\nplt.show()\n\n# confirm imbalance\nprint('Original dataset shape:', Counter(y))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:17:36.948449Z","iopub.execute_input":"2023-05-21T17:17:36.950218Z","iopub.status.idle":"2023-05-21T17:17:36.964417Z","shell.execute_reply.started":"2023-05-21T17:17:36.950169Z","shell.execute_reply":"2023-05-21T17:17:36.963069Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# SMOTE\nsm = SMOTE(sampling_strategy=0.25)\nX_res, y_res = sm.fit_resample(X, y)\n\n# plot the dataset after SMOTE\nplt.scatter(X_res[:, 0], X_res[:, 1], marker='o', c=y_res,\n            s=25, edgecolor='k')\n\nplt.show()\n\n# confirm oversampling\nprint('Resampled (SMOTE) dataset shape:', Counter(y_res))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:17:37.076002Z","iopub.execute_input":"2023-05-21T17:17:37.078641Z","iopub.status.idle":"2023-05-21T17:17:37.082843Z","shell.execute_reply.started":"2023-05-21T17:17:37.078587Z","shell.execute_reply":"2023-05-21T17:17:37.081973Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"# RandomUnderSampler\nrus = RandomUnderSampler(sampling_strategy=3/4)\nX_res_under, y_res_under = rus.fit_resample(X_res, y_res)\n\n# plot the dataset after RandomUnderSampler\nplt.scatter(X_res_under[:, 0], X_res_under[:, 1], marker='o', c=y_res_under,\n            s=25, edgecolor='k')\n\nplt.show()\n\n# confirm undersampling\nprint('Resampled (RandomUnderSampler) dataset shape:', Counter(y_res_under))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:17:37.084533Z","iopub.execute_input":"2023-05-21T17:17:37.085736Z","iopub.status.idle":"2023-05-21T17:17:37.097694Z","shell.execute_reply.started":"2023-05-21T17:17:37.085677Z","shell.execute_reply":"2023-05-21T17:17:37.095967Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"# Question 6\n\nWrite a Python code to perform data preprocessing on a dataset using the scikit-learn library. Follow the instructions below:\n\n * Load the dataset using the scikit-learn `load_iris` function.\n * Assign the feature data to a variable named `X` and the target data to a variable named `y`.\n * Create a pandas DataFrame called `df` using `X` as the data and the feature names obtained from the dataset.\n * Display the first 5 rows of the DataFrame `df`.\n *  Check if there are any missing values in the DataFrame and handle them accordingly.\n * Split the data into training and testing sets using the `train_test_split` function from scikit-learn. Assign 70% of the data to the training set and the remaining 30% to the testing set.\n * Print the dimensions of the training set and testing set respectively.\n *  Standardize the feature data in the training set using the `StandardScaler` from scikit-learn.\n *  Apply the same scaling transformation on the testing set.\n * Print the first 5 rows of the standardized training set.","metadata":{}},{"cell_type":"code","source":"# Import necessary modules\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\n# Load dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Create DataFrame\ndf = pd.DataFrame(X, columns=data.feature_names)\n\n# Display the first 5 rows of the DataFrame df\nprint(df.head())\n\n# Check if there are any missing values in the DataFrame and handle them accordingly\nif df.isnull().sum().sum() > 0:\n    print(\"Found missing values!\")\n    df = df.fillna(df.mean())  # fill with mean of the column\nelse:\n    print(\"No missing values found.\")\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.30, random_state=42)\n\n# Print the dimensions of the training set and testing set respectively\nprint(\"Train set dimensions: \", X_train.shape)\nprint(\"Test set dimensions: \", X_test.shape)\n\n# Standardize the feature data in the training set\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\n# Apply the same scaling transformation on the testing set\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n\n# Print the first 5 rows of the standardized training set\nprint(X_train_scaled.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:17:37.099512Z","iopub.execute_input":"2023-05-21T17:17:37.100781Z","iopub.status.idle":"2023-05-21T17:17:37.139871Z","shell.execute_reply.started":"2023-05-21T17:17:37.100732Z","shell.execute_reply":"2023-05-21T17:17:37.138257Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\nNo missing values found.\nTrain set dimensions:  (105, 4)\nTest set dimensions:  (45, 4)\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0          -0.413416         -1.462003          -0.099511         -0.323398\n1           0.551222         -0.502563           0.717703          0.353032\n2           0.671802          0.217016           0.951192          0.758890\n3           0.912961         -0.022844           0.309096          0.217746\n4           1.636440          1.416315           1.301427          1.705891\n","output_type":"stream"}]}]}